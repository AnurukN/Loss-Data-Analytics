<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Loss Data Analytics</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience.">
  <meta name="generator" content="bookdown 0.3 and GitBook 2.6.7">

  <meta property="og:title" content="Loss Data Analytics" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Loss Data Analytics" />
  
  <meta name="twitter:description" content="Loss Data Analytics is an interactive, online, freely available text. - The online version will contain many interactive objects (quizzes, computer demonstrations, interactive graphs, video, and the like) to promote deeper learning. - A subset of the book will be available in pdf format for low-cost printing. - The online text will be available in multiple languages to promote access to a worldwide audience." />
  

<meta name="author" content="An open text authored by the Actuarial Community">


<meta name="date" content="2017-06-27">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="modeling-loss-severity.html">
<link rel="next" href="simulation.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script language="javascript">
function toggle(id1,id2) {    var ele = document.getElementById(id1);    var text = document.getElementById(id2);    if (ele.style.display == "block") {       ele.style.display = "none";       text.innerHTML = "Show Solution";    } else {       ele.style.display = "block";       text.innerHTML = "Hide Solution";    } }
</script>
<script language="javascript">function togglecode(id1,id2) {
   var ele = document.getElementById(id1);    var text = document.getElementById(id2);
   if (ele.style.display == "block") {      ele.style.display = "none";      text.innerHTML = "Show R Code";   } 
      else {  ele.style.display = "block";   text.innerHTML = "Hide R Code";   }}		
</script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Loss Data Analytics</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html"><i class="fa fa-check"></i><b>1</b> Introduction to Loss Data Analytics</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#S:Intro"><i class="fa fa-check"></i><b>1.1</b> Relevance of Analytics</a><ul>
<li class="chapter" data-level="1.1.1" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#what-is-analytics"><i class="fa fa-check"></i><b>1.1.1</b> What is Analytics?</a></li>
<li class="chapter" data-level="1.1.2" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#short-term-insurance"><i class="fa fa-check"></i><b>1.1.2</b> Short-term Insurance</a></li>
<li class="chapter" data-level="1.1.3" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#S:InsProcesses"><i class="fa fa-check"></i><b>1.1.3</b> Insurance Processes</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#variable-types"><i class="fa fa-check"></i><b>1.2</b> Variable Types</a><ul>
<li class="chapter" data-level="1.2.1" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#qualitative-variables"><i class="fa fa-check"></i><b>1.2.1</b> Qualitative Variables</a></li>
<li class="chapter" data-level="1.2.2" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#quantitative-variables"><i class="fa fa-check"></i><b>1.2.2</b> Quantitative Variables</a></li>
<li class="chapter" data-level="1.2.3" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#multivariate-variables"><i class="fa fa-check"></i><b>1.2.3</b> Multivariate Variables</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#S:PredModApps"><i class="fa fa-check"></i><b>1.3</b> Insurance Company Operations</a><ul>
<li class="chapter" data-level="1.3.1" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#initiating-insurance"><i class="fa fa-check"></i><b>1.3.1</b> Initiating Insurance</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#renewing-insurance"><i class="fa fa-check"></i><b>1.3.2</b> Renewing Insurance</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#claims-and-product-management"><i class="fa fa-check"></i><b>1.3.3</b> Claims and Product Management</a></li>
<li class="chapter" data-level="1.3.4" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#S:Reserving"><i class="fa fa-check"></i><b>1.3.4</b> Loss Reserving</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#S:LGPIF"><i class="fa fa-check"></i><b>1.4</b> Case Study: Wisconsin Property Fund</a><ul>
<li class="chapter" data-level="1.4.1" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#S:OutComes"><i class="fa fa-check"></i><b>1.4.1</b> Fund Claims Variables</a></li>
<li class="chapter" data-level="1.4.2" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#S:FundVariables"><i class="fa fa-check"></i><b>1.4.2</b> Fund Rating Variables</a></li>
<li class="chapter" data-level="1.4.3" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#fund-operations"><i class="fa fa-check"></i><b>1.4.3</b> Fund Operations</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="introduction-to-loss-data-analytics.html"><a href="introduction-to-loss-data-analytics.html#further-reading-and-resources"><i class="fa fa-check"></i><b>1.5</b> Further Resources and Contributors</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="frequency-distributions.html"><a href="frequency-distributions.html"><i class="fa fa-check"></i><b>2</b> Frequency Distributions</a><ul>
<li class="chapter" data-level="2.1" data-path="frequency-distributions.html"><a href="frequency-distributions.html#how-frequency-augments-severity-information"><i class="fa fa-check"></i><b>2.1</b> How Frequency Augments Severity Information</a></li>
<li class="chapter" data-level="2.2" data-path="frequency-distributions.html"><a href="frequency-distributions.html#basic-frequency-distributions"><i class="fa fa-check"></i><b>2.2</b> Basic Frequency Distributions</a><ul>
<li class="chapter" data-level="2.2.1" data-path="frequency-distributions.html"><a href="frequency-distributions.html#foundations"><i class="fa fa-check"></i><b>2.2.1</b> Foundations</a></li>
<li class="chapter" data-level="2.2.2" data-path="frequency-distributions.html"><a href="frequency-distributions.html#probability-generating-function"><i class="fa fa-check"></i><b>2.2.2</b> Probability Generating Function</a></li>
<li class="chapter" data-level="2.2.3" data-path="frequency-distributions.html"><a href="frequency-distributions.html#important-frequency-distributions"><i class="fa fa-check"></i><b>2.2.3</b> Important Frequency Distributions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="frequency-distributions.html"><a href="frequency-distributions.html#the-a-b-0-class"><i class="fa fa-check"></i><b>2.3</b> The (<span class="math inline">\(a, b\)</span>, 0) Class</a><ul>
<li class="chapter" data-level="2.3.1" data-path="frequency-distributions.html"><a href="frequency-distributions.html#the-a-b-0-class---example"><i class="fa fa-check"></i><b>2.3.1</b> The (<span class="math inline">\(a, b\)</span>, 0) Class - Example</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="frequency-distributions.html"><a href="frequency-distributions.html#estimating-frequency-distributions"><i class="fa fa-check"></i><b>2.4</b> Estimating Frequency Distributions</a></li>
<li class="chapter" data-level="2.5" data-path="frequency-distributions.html"><a href="frequency-distributions.html#other-frequency-distributions"><i class="fa fa-check"></i><b>2.5</b> Other Frequency Distributions</a><ul>
<li class="chapter" data-level="2.5.1" data-path="frequency-distributions.html"><a href="frequency-distributions.html#zero-truncation-or-modification"><i class="fa fa-check"></i><b>2.5.1</b> Zero Truncation or Modification</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="frequency-distributions.html"><a href="frequency-distributions.html#mixture-distributions"><i class="fa fa-check"></i><b>2.6</b> Mixture Distributions</a><ul>
<li class="chapter" data-level="2.6.1" data-path="frequency-distributions.html"><a href="frequency-distributions.html#mixtures-of-finite-populations"><i class="fa fa-check"></i><b>2.6.1</b> Mixtures of Finite Populations</a></li>
<li class="chapter" data-level="2.6.2" data-path="frequency-distributions.html"><a href="frequency-distributions.html#mixtures-of-infinitely-many-populations"><i class="fa fa-check"></i><b>2.6.2</b> Mixtures of Infinitely Many Populations</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="frequency-distributions.html"><a href="frequency-distributions.html#goodness-of-fit"><i class="fa fa-check"></i><b>2.7</b> Goodness of Fit</a></li>
<li class="chapter" data-level="2.8" data-path="frequency-distributions.html"><a href="frequency-distributions.html#exercises"><i class="fa fa-check"></i><b>2.8</b> Exercises</a></li>
<li class="chapter" data-level="2.9" data-path="frequency-distributions.html"><a href="frequency-distributions.html#technical-supplement-iterated-expectations"><i class="fa fa-check"></i><b>2.9</b> Technical Supplement: Iterated Expectations</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html"><i class="fa fa-check"></i><b>3</b> Modeling Loss Severity</a><ul>
<li class="chapter" data-level="3.1" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#BasicQuantities"><i class="fa fa-check"></i><b>3.1</b> Basic Distributional Quantities</a><ul>
<li class="chapter" data-level="3.1.1" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#moments"><i class="fa fa-check"></i><b>3.1.1</b> Moments</a></li>
<li class="chapter" data-level="3.1.2" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#quantiles"><i class="fa fa-check"></i><b>3.1.2</b> Quantiles</a></li>
<li class="chapter" data-level="3.1.3" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#the-moment-generating-function"><i class="fa fa-check"></i><b>3.1.3</b> The Moment Generating Function</a></li>
<li class="chapter" data-level="3.1.4" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#probability-generating-function-1"><i class="fa fa-check"></i><b>3.1.4</b> Probability Generating Function</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#ContinuousDistn"><i class="fa fa-check"></i><b>3.2</b> Continuous Distributions for Modeling Loss Severity</a><ul>
<li class="chapter" data-level="3.2.1" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#the-gamma-distribution"><i class="fa fa-check"></i><b>3.2.1</b> The Gamma Distribution</a></li>
<li class="chapter" data-level="3.2.2" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#the-pareto-distribution"><i class="fa fa-check"></i><b>3.2.2</b> The Pareto Distribution</a></li>
<li class="chapter" data-level="3.2.3" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#the-weibull-distribution"><i class="fa fa-check"></i><b>3.2.3</b> The Weibull Distribution</a></li>
<li class="chapter" data-level="3.2.4" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#the-generalized-beta-distribution-of-the-second-kind"><i class="fa fa-check"></i><b>3.2.4</b> The Generalized Beta Distribution of the Second Kind</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#MethodsCreation"><i class="fa fa-check"></i><b>3.3</b> Methods of Creating New Distributions</a><ul>
<li class="chapter" data-level="3.3.1" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#functions-of-random-variables-and-their-distributions"><i class="fa fa-check"></i><b>3.3.1</b> Functions of Random Variables and their Distributions</a></li>
<li class="chapter" data-level="3.3.2" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#multiplication-by-a-constant"><i class="fa fa-check"></i><b>3.3.2</b> Multiplication by a Constant</a></li>
<li class="chapter" data-level="3.3.3" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#raising-to-a-power"><i class="fa fa-check"></i><b>3.3.3</b> Raising to a Power</a></li>
<li class="chapter" data-level="3.3.4" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#exponentiation"><i class="fa fa-check"></i><b>3.3.4</b> Exponentiation</a></li>
<li class="chapter" data-level="3.3.5" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#finite-mixtures"><i class="fa fa-check"></i><b>3.3.5</b> Finite Mixtures</a></li>
<li class="chapter" data-level="3.3.6" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#continuous-mixtures"><i class="fa fa-check"></i><b>3.3.6</b> Continuous Mixtures</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#coverage-modifications"><i class="fa fa-check"></i><b>3.4</b> Coverage Modifications</a><ul>
<li class="chapter" data-level="3.4.1" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#PolicyDeduct"><i class="fa fa-check"></i><b>3.4.1</b> Policy Deductibles</a></li>
<li class="chapter" data-level="3.4.2" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#PolicyLimits"><i class="fa fa-check"></i><b>3.4.2</b> Policy Limits</a></li>
<li class="chapter" data-level="3.4.3" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#coinsurance"><i class="fa fa-check"></i><b>3.4.3</b> Coinsurance</a></li>
<li class="chapter" data-level="3.4.4" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#reinsurance"><i class="fa fa-check"></i><b>3.4.4</b> Reinsurance</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>3.5</b> Maximum Likelihood Estimation</a><ul>
<li class="chapter" data-level="3.5.1" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#maximum-likelihood-estimators-for-complete-data"><i class="fa fa-check"></i><b>3.5.1</b> Maximum Likelihood Estimators for Complete Data</a></li>
<li class="chapter" data-level="3.5.2" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#MLEGrouped"><i class="fa fa-check"></i><b>3.5.2</b> Maximum Likelihood Estimators for Grouped Data</a></li>
<li class="chapter" data-level="3.5.3" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#maximum-likelihood-estimators-for-censored-data"><i class="fa fa-check"></i><b>3.5.3</b> Maximum Likelihood Estimators for Censored Data</a></li>
<li class="chapter" data-level="3.5.4" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#maximum-likelihood-estimators-for-truncated-data"><i class="fa fa-check"></i><b>3.5.4</b> Maximum Likelihood Estimators for Truncated Data</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#Resources-loss-severity"><i class="fa fa-check"></i><b>3.6</b> Further Resources and Contributors</a></li>
<li class="chapter" data-level="3.7" data-path="modeling-loss-severity.html"><a href="modeling-loss-severity.html#exercises-1"><i class="fa fa-check"></i><b>3.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html"><i class="fa fa-check"></i><b>4</b> Model Selection and Inference</a><ul>
<li class="chapter" data-level="4.1" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#S:NonParTools"><i class="fa fa-check"></i><b>4.1</b> Nonparametric Estimation Tools</a><ul>
<li class="chapter" data-level="4.1.1" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#moments-1"><i class="fa fa-check"></i><b>4.1.1</b> Moments</a></li>
<li class="chapter" data-level="4.1.2" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#quantiles-1"><i class="fa fa-check"></i><b>4.1.2</b> Quantiles</a></li>
<li class="chapter" data-level="4.1.3" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#density-estimators"><i class="fa fa-check"></i><b>4.1.3</b> Density Estimators</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#nonparametric-estimation-tools-for-model-selection"><i class="fa fa-check"></i><b>4.2</b> Nonparametric Estimation Tools For Model Selection</a><ul>
<li class="chapter" data-level="4.2.1" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#graphical-comparisions"><i class="fa fa-check"></i><b>4.2.1</b> Graphical Comparisions</a></li>
<li class="chapter" data-level="4.2.2" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#statistical-comparisions"><i class="fa fa-check"></i><b>4.2.2</b> Statistical Comparisions</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#nonparametric-estimation-using-modified-data"><i class="fa fa-check"></i><b>4.3</b> Nonparametric Estimation using Modified Data</a><ul>
<li class="chapter" data-level="4.3.1" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#grouped-data"><i class="fa fa-check"></i><b>4.3.1</b> Grouped Data</a></li>
<li class="chapter" data-level="4.3.2" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#censored-data"><i class="fa fa-check"></i><b>4.3.2</b> Censored Data</a></li>
<li class="chapter" data-level="4.3.3" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#truncated-data"><i class="fa fa-check"></i><b>4.3.3</b> Truncated Data</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#topics-in-parametric-estimation"><i class="fa fa-check"></i><b>4.4</b> Topics in Parametric Estimation</a><ul>
<li class="chapter" data-level="4.4.1" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#starting-values"><i class="fa fa-check"></i><b>4.4.1</b> Starting Values</a></li>
<li class="chapter" data-level="4.4.2" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#grouped-data-2"><i class="fa fa-check"></i><b>4.4.2</b> Grouped Data</a></li>
<li class="chapter" data-level="4.4.3" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#parametric-estimation-using-censored-data"><i class="fa fa-check"></i><b>4.4.3</b> Parametric Estimation Using Censored Data</a></li>
<li class="chapter" data-level="4.4.4" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#censored-and-truncated-data"><i class="fa fa-check"></i><b>4.4.4</b> Censored and Truncated Data</a></li>
<li class="chapter" data-level="4.4.5" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#parametric-estimation-using-censored-and-truncated-data"><i class="fa fa-check"></i><b>4.4.5</b> Parametric Estimation Using Censored and Truncated Data</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#bayesian-inference"><i class="fa fa-check"></i><b>4.5</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="4.5.1" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#bayesian-model"><i class="fa fa-check"></i><b>4.5.1</b> Bayesian Model</a></li>
<li class="chapter" data-level="4.5.2" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#bayesian-inference---decision-analysis"><i class="fa fa-check"></i><b>4.5.2</b> Bayesian Inference - Decision Analysis</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="model-selection-and-inference.html"><a href="model-selection-and-inference.html#exercises-2"><i class="fa fa-check"></i><b>4.6</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="simulation.html"><a href="simulation.html"><i class="fa fa-check"></i><b>5</b> Simulation</a><ul>
<li class="chapter" data-level="5.1" data-path="simulation.html"><a href="simulation.html#generating-independent-uniform-observations"><i class="fa fa-check"></i><b>5.1</b> Generating Independent Uniform Observations</a></li>
<li class="chapter" data-level="5.2" data-path="simulation.html"><a href="simulation.html#inverse-transform"><i class="fa fa-check"></i><b>5.2</b> Inverse Transform</a></li>
<li class="chapter" data-level="5.3" data-path="simulation.html"><a href="simulation.html#how-many-simulated-values"><i class="fa fa-check"></i><b>5.3</b> How Many Simulated Values?</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="portfolio-management-including-reinsurance.html"><a href="portfolio-management-including-reinsurance.html"><i class="fa fa-check"></i><b>6</b> Portfolio Management including Reinsurance</a><ul>
<li class="chapter" data-level="6.0.1" data-path="portfolio-management-including-reinsurance.html"><a href="portfolio-management-including-reinsurance.html#overview"><i class="fa fa-check"></i><b>6.0.1</b> Overview:</a></li>
<li class="chapter" data-level="6.1" data-path="portfolio-management-including-reinsurance.html"><a href="portfolio-management-including-reinsurance.html#tails-of-distributions"><i class="fa fa-check"></i><b>6.1</b> Tails of Distributions</a></li>
<li class="chapter" data-level="6.2" data-path="portfolio-management-including-reinsurance.html"><a href="portfolio-management-including-reinsurance.html#measures-of-risk"><i class="fa fa-check"></i><b>6.2</b> Measures of Risk</a></li>
<li class="chapter" data-level="6.3" data-path="portfolio-management-including-reinsurance.html"><a href="portfolio-management-including-reinsurance.html#reinsurance-1"><i class="fa fa-check"></i><b>6.3</b> Reinsurance</a><ul>
<li class="chapter" data-level="6.3.1" data-path="portfolio-management-including-reinsurance.html"><a href="portfolio-management-including-reinsurance.html#proportional-reinsurance"><i class="fa fa-check"></i><b>6.3.1</b> Proportional Reinsurance</a></li>
<li class="chapter" data-level="6.3.2" data-path="portfolio-management-including-reinsurance.html"><a href="portfolio-management-including-reinsurance.html#surplus-share-proportional-treaty"><i class="fa fa-check"></i><b>6.3.2</b> Surplus Share Proportional Treaty</a></li>
<li class="chapter" data-level="6.3.3" data-path="portfolio-management-including-reinsurance.html"><a href="portfolio-management-including-reinsurance.html#excess-of-loss-reinsurance"><i class="fa fa-check"></i><b>6.3.3</b> Excess of Loss Reinsurance</a></li>
<li class="chapter" data-level="6.3.4" data-path="portfolio-management-including-reinsurance.html"><a href="portfolio-management-including-reinsurance.html#relations-with-personal-insurance"><i class="fa fa-check"></i><b>6.3.4</b> Relations with Personal Insurance</a></li>
<li class="chapter" data-level="6.3.5" data-path="portfolio-management-including-reinsurance.html"><a href="portfolio-management-including-reinsurance.html#layers-of-coverage"><i class="fa fa-check"></i><b>6.3.5</b> Layers of Coverage</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="technical-supplement-statistical-inference.html"><a href="technical-supplement-statistical-inference.html"><i class="fa fa-check"></i><b>7</b> Technical Supplement: Statistical Inference</a><ul>
<li class="chapter" data-level="7.1" data-path="technical-supplement-statistical-inference.html"><a href="technical-supplement-statistical-inference.html#overview-of-statistical-inference"><i class="fa fa-check"></i><b>7.1</b> Overview of Statistical Inference</a></li>
<li class="chapter" data-level="7.2" data-path="technical-supplement-statistical-inference.html"><a href="technical-supplement-statistical-inference.html#estimation-and-prediction"><i class="fa fa-check"></i><b>7.2</b> Estimation and Prediction</a></li>
<li class="chapter" data-level="7.3" data-path="technical-supplement-statistical-inference.html"><a href="technical-supplement-statistical-inference.html#maximum-likelihood-theory"><i class="fa fa-check"></i><b>7.3</b> Maximum Likelihood Theory</a><ul>
<li class="chapter" data-level="7.3.1" data-path="technical-supplement-statistical-inference.html"><a href="technical-supplement-statistical-inference.html#likelihood-function"><i class="fa fa-check"></i><b>7.3.1</b> Likelihood Function</a></li>
<li class="chapter" data-level="7.3.2" data-path="technical-supplement-statistical-inference.html"><a href="technical-supplement-statistical-inference.html#information-criteria"><i class="fa fa-check"></i><b>7.3.2</b> Information Criteria</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Loss Data Analytics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="model-selection-and-inference" class="section level1">
<h1><span class="header-section-number">Chapter 4</span> Model Selection and Inference</h1>
<div id="S:NonParTools" class="section level2">
<h2><span class="header-section-number">4.1</span> Nonparametric Estimation Tools</h2>
<div id="moments-1" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Moments</h3>
<div id="moment-estimators" class="section level4">
<h4><span class="header-section-number">4.1.1.1</span> Moment Estimators</h4>
<ul>
<li><p><span class="math inline">\(X_1, \ldots, X_n\)</span> is a random sample (with replacement) from F(.)</p></li>
<li><p>Sometimes we say that <span class="math inline">\(X_1, \ldots, X_n\)</span> are identically and independently distributed (<span class="math inline">\(iid\)</span>)</p></li>
</ul>
<p>We will not assume a parametric form for the distribution function F() and so proceed with a <em>nonparametric</em> analysis.</p>
<ul>
<li><p>The <span class="math inline">\(k\)</span>th (<em>raw</em>) moment is <span class="math inline">\(\mathrm{E~} X^k = \mu^{\prime}_k\)</span> .</p></li>
<li><p>It is estimated by the corresponding statistic <span class="math display">\[\frac{1}{n} \sum_{i=1}^n X_i^k .\]</span></p></li>
<li><p>The <span class="math inline">\(k\)</span>th (central) moment is <span class="math inline">\(\mathrm{E~} (X-\mu)^k = \mu_k\)</span>.</p></li>
<li><p>It is estimated by <span class="math display">\[\frac{1}{n} \sum_{i=1}^n \left(X_i-\bar{X}\right)^k .\]</span></p></li>
</ul>
</div>
<div id="empirical-distribution-function" class="section level4">
<h4><span class="header-section-number">4.1.1.2</span> Empirical Distribution Function</h4>
<ul>
<li><p>Define the <strong>empirical distribution function</strong> to be <span class="math display">\[\begin{aligned}
F_n(x) &amp;= \frac{\text{number of observations less than or equal to }x}{n} \\
&amp;= \frac{1}{n} \sum_{i=1}^n I\left(X_i \le x\right).\end{aligned}\]</span> Here, the notation <span class="math inline">\(I(\cdot)\)</span> is the indicator function, it returns 1 if the event <span class="math inline">\((\cdot)\)</span> is true and 0 otherwise.</p></li>
<li><p><strong>Example – Toy</strong>. Consider <span class="math inline">\(n=10\)</span> observations as in Figure <a href="model-selection-and-inference.html#fig:EDFToy">4.1</a></p></li>
</ul>
<span class="math display">\[\begin{equation*}
\begin{array}{l|cccccccccc}
    \hline
i   &amp;1&amp;2&amp;3&amp;4&amp;5&amp;6&amp;7&amp;8&amp;9&amp;10 \\
X_i &amp; 10 &amp;15 &amp;15 &amp;15 &amp;20 &amp;23 &amp;23 &amp;23 &amp;23 &amp;30\\
    \hline
    \end{array}\end{equation*}\]</span>
<ul>
<li><span class="math inline">\(\bar{x} = 19.7\)</span> and that the estimate of the second central moment, the <strong>sample variance</strong>, is 34.45556.</li>
</ul>
<pre><code>##  [1] 10 15 15 15 20 23 23 23 23 30</code></pre>
<div class="figure" style="text-align: center"><span id="fig:EDFToy"></span>
<img src="LossDataAnalytics_files/figure-html/EDFToy-1.png" alt="Empirical Distribution Function of a Toy Example" width="80%" />
<p class="caption">
Figure 4.1: Empirical Distribution Function of a Toy Example
</p>
</div>
<h4 style="text-align: center;">
<a id="displayText2.4f" href="javascript:togglecode('toggleToy','displayText2.4f');"><i><strong>R Code for Toy Example CDF</strong></i></a>
</h4>
<div id="toggleToy" style="display: none">
<pre><code>(xExample = c(10,rep(15,3),20,rep(23,4),30))
PercentilesxExample &lt;- ecdf(xExample)
plot(PercentilesxExample, main=&quot;&quot;,xlab=&quot;x&quot;)</code></pre>
</div>
</div>
</div>
<div id="quantiles-1" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Quantiles</h3>
<ul>
<li><p>Special Cases</p>
<ul>
<li><p>The <em>median</em> is that point so that approximately half of a data set is below (or above) it.</p></li>
<li><p>The first <em>quartile</em> is that number so that approximately 25% of the data is below it.</p></li>
<li><p>A <span class="math inline">\(100p\)</span> <em>percentile</em> is that number so that <span class="math inline">\(100 \times p\)</span> percent of the data is below it.</p></li>
</ul></li>
<li><p>In general, for a given <span class="math inline">\(0&lt;q&lt;1\)</span>, define the <strong><span class="math inline">\(q\)</span>th quantile</strong> <span class="math inline">\(q_F\)</span> to be any number that satisfies <span class="math display">\[\begin{aligned}
\label{E:Quantile}
F(q_F-) \le q \le F(q_F).\end{aligned}\]</span> Here, the notation <span class="math inline">\(F(x-)\)</span> means to evaluate the function <span class="math inline">\(F(\cdot)\)</span> as a left-hand limit.</p></li>
<li><p>If <span class="math inline">\(F(\cdot)\)</span> is continuous at <span class="math inline">\(q_F\)</span>, then <span class="math inline">\(F(q_F-) = F(q_F)\)</span></p></li>
</ul>
<div class="figure">
<img src="FiguresCh4/ContinuousQuantileCase.jpg" alt="Quantiles for a Continuous Distribution Function" />
<p class="caption">Quantiles for a Continuous Distribution Function</p>
</div>
<div id="quantiles-2" class="section level4">
<h4><span class="header-section-number">4.1.2.1</span> Quantiles</h4>
<ul>
<li><p>If F is smooth or there is a jump at <span class="math inline">\(q\)</span>, the definition of the quantile <span class="math inline">\(q_F\)</span> is unique</p></li>
<li><p>if F is flat at <span class="math inline">\(q\)</span>, then there a many definitions of <span class="math inline">\(q_F\)</span></p></li>
</ul>
<div class="figure">
<img src="FiguresCh4/ThreeQuantileCases.jpg" alt="Three Quantile Cases" />
<p class="caption">Three Quantile Cases</p>
</div>
</div>
<div id="quantiles-3" class="section level4">
<h4><span class="header-section-number">4.1.2.2</span> Quantiles</h4>
<p><strong>Example – Toy</strong>. Consider <span class="math inline">\(n=10\)</span> observations:</p>
<ul>
<li><p>The median might be defined to be any number between 20 and 23.</p></li>
<li><p>Many software packages use the average 21.5.</p></li>
<li><p>KPW defines the <em>smoothed empirical percentile</em> to be <span class="math display">\[\hat{\pi}_q = (1-h) X_{(j)} + h X_{(j+1)}\]</span> where <span class="math inline">\(j=[(n+1)q]\)</span> and, <span class="math inline">\(h=(n+1)q-j\)</span>, and <span class="math inline">\(X_{(1)}, \ldots, X_{(n)}\)</span> are the ordered values (the <em>order statistics</em>) corresponding to <span class="math inline">\(X_1, \ldots, X_n\)</span>.</p></li>
</ul>
<p><strong>Example</strong>. Take <span class="math inline">\(n=10\)</span> and <span class="math inline">\(q=0.5\)</span>. Then, <span class="math inline">\(j=[(11)0.5]=[5.5]=5\)</span> and, <span class="math inline">\(h=(11)(0.5)-5=0.5\)</span>. With this <span class="math display">\[\hat{\pi}_{0.5} = (1-0.5) X_{(5)} + (0.5) X_{(6)} = 0.5 (20) + (0.5)(23) = 21.5.\]</span> Take <span class="math inline">\(n=10\)</span> and <span class="math inline">\(q=0.2\)</span>. Then, <span class="math inline">\(j=[(11)0.2]=[2.2]=2\)</span> and, <span class="math inline">\(h=(11)(0.2)-2=0.2\)</span>. With this <span class="math display">\[\hat{\pi}_{0.2} = (1-0.2) X_{(2)} + (0.2) X_{(3)} = 0.2 (15) + (0.8)(15) = 15.\]</span></p>
</div>
</div>
<div id="density-estimators" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Density Estimators</h3>
<ul>
<li><p>When the random variable is discrete, estimate the probability mass function <span class="math inline">\(f(x) = \Pr(X=x)\)</span> is using <span class="math display">\[f_n(x) = \frac{1}{n} \sum_{i=1}^n I(X_i = x).\]</span></p></li>
<li><p>Observations may be grouped in the sense that they fall into intervals of the form <span class="math inline">\([c_{j-1}, c_j)\)</span>, for <span class="math inline">\(j=1, \ldots, k\)</span>. The constants <span class="math inline">\(\{c_0 &lt; c_1 &lt; \cdots &lt; c_k\}\)</span> form some partition of the domain of F(.).</p></li>
<li><p>Then, use <span class="math display">\[f_n(x) = \frac{n_j}{n \times (c_j - c_{j-1})}  \ \ \ \ \ \ c_{j-1} \le x &lt; c_j,\]</span> where <span class="math inline">\(n_j\)</span> is the number of observations (<span class="math inline">\(X_i\)</span>) that fall into the interval <span class="math inline">\([c_{j-1}, c_j)\)</span>.</p></li>
<li><p>Another way to write this is <span class="math display">\[f_n(x) = \frac{1}{n(c_j-c_{j-1})} \sum_{i=1}^n I(c_{j-1} &lt; X_i \le c_j).\]</span></p></li>
</ul>
<div id="uniform-kernel-density-estimator" class="section level4">
<h4><span class="header-section-number">4.1.3.1</span> Uniform Kernel Density Estimator</h4>
<ul>
<li><p>Let <span class="math inline">\(b&gt;0\)</span>, known as a bandwidth, <span class="math display">\[\begin{aligned}
\label{E:KDF}
 f_n(x) = \frac{1}{2nb} \sum_{i=1}^n I(x-b &lt; X_i \le x + b).\end{aligned}\]</span></p></li>
<li><p>The estimator is the average over <span class="math inline">\(n\)</span> <span class="math inline">\(iid\)</span> realizations of a random variable with mean <span class="math display">\[\begin{aligned}
\mathrm{E~ } \frac{1}{2b} I(x-b &lt; X \le x + b) &amp;= \frac{1}{2b}\left(F(x+b)-F(x-b)\right) \\
&amp;= \frac{1}{2b} \left( \left\{ F(x) + b F^{\prime}(x) + b^2 C_1\right\} \right.\\
&amp; ~ ~ ~ -
\left. \left\{ F(x) - b F^{\prime}(x) + b^2 C_2\right\} \right) \\
&amp;= F^{\prime}(x) + b \frac{C_1-C_2}{2} \rightarrow  F^{\prime}(x) = f(x),\end{aligned}\]</span> as <span class="math inline">\(b\rightarrow 0\)</span>. That is, <span class="math inline">\(f_n(x)\)</span> is an asymptotically unbiased estimator of <span class="math inline">\(f(x)\)</span>.</p></li>
</ul>
</div>
<div id="kernel-density-estimator" class="section level4">
<h4><span class="header-section-number">4.1.3.2</span> Kernel Density Estimator</h4>
<ul>
<li><p>More generally, define the <strong>kernel density estimator</strong> <span class="math display">\[\begin{aligned}
\label{E:KDF2}
 f_n(x) = \frac{1}{nb} \sum_{i=1}^n k\left(\frac{x-X_i}{b}\right).\end{aligned}\]</span> where <span class="math inline">\(k\)</span> is a probability density function centered about 0.</p></li>
<li><p>Special Cases</p>
<ul>
<li><p>uniform kernel, <span class="math inline">\(k(y) = \frac{1}{2}I(-1 &lt; y \leq 1)\)</span> .</p></li>
<li><p>triangular kernel, <span class="math inline">\(k(y) = (1-|y|)\times I(|y| \le 1)\)</span></p></li>
<li><p>Epanechnikov kernel, <span class="math inline">\(k(y) = \frac{3}{4}(1-y^2) \times I(|y| \le 1)\)</span>, and</p></li>
<li><p>Gaussian kernel <span class="math inline">\(k(y) = \phi(y)\)</span>, where <span class="math inline">\(\phi(\cdot)\)</span> is the standard normal density function.</p></li>
</ul></li>
</ul>
</div>
<div id="kernel-density-estimator-of-a-distribution-function" class="section level4">
<h4><span class="header-section-number">4.1.3.3</span> Kernel Density Estimator of a Distribution Function</h4>
<ul>
<li><p>The kernel density estimator of a <strong>distribution function</strong> is <span class="math display">\[\begin{aligned}
 \hat{F}_n(x) = \frac{1}{n} \sum_{i=1}^n K\left(\frac{x-X_i}{b}\right).\end{aligned}\]</span> where <span class="math inline">\(K\)</span> is a probability distribution function associated with the kernel density <span class="math inline">\(k\)</span>.</p></li>
<li><p>To illustrate, for the uniform kernel, we have <span class="math inline">\(k(y) = \frac{1}{2}I(-1 &lt; y \le 1)\)</span> so <span class="math display">\[\begin{aligned}
K(y) =
\begin{cases}
0 &amp;            y&lt;-1\\
\frac{y+1}{2}&amp; -1 \le y &lt; 1 \\
1 &amp; y \ge 1 \\
\end{cases}\end{aligned}\]</span></p></li>
</ul>
</div>
</div>
</div>
<div id="nonparametric-estimation-tools-for-model-selection" class="section level2">
<h2><span class="header-section-number">4.2</span> Nonparametric Estimation Tools For Model Selection</h2>
<div id="graphical-comparisions" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Graphical Comparisions</h3>
<div id="comparing-distribution-and-density-functions" class="section level4">
<h4><span class="header-section-number">4.2.1.1</span> Comparing Distribution and Density Functions</h4>
<ul>
<li><p>The left-hand panel compares distribution functions, with the dots corresponding to the empirical distribution, the thick blue curve corresponding to the fitted gamma and the light purple curve corresponding to the fitted Pareto.</p></li>
<li><p>The right hand panel compares these three distributions summarized using probability density functions.</p></li>
</ul>
<div class="figure">
<img src="FiguresCh4/ComparisonCDFPDF.jpg" alt="Nonparametric Versus Fitted Parametric Distribution and Density Functions" />
<p class="caption">Nonparametric Versus Fitted Parametric Distribution and Density Functions</p>
</div>
</div>
<div id="pp-plot" class="section level4">
<h4><span class="header-section-number">4.2.1.2</span> <em>PP</em> Plot</h4>
<ul>
<li><p>The horizontal axes gives the empirical distribution function at each observation.</p></li>
<li><p>In the left-hand panel, the corresponding distribution function for the gamma is shown in the vertical axis.</p></li>
<li><p>The right-hand panel shows the fitted Pareto distribution. Lines of <span class="math inline">\(y=x\)</span> are superimposed.</p></li>
</ul>
<div class="figure">
<img src="FiguresCh4/PPPlot.jpg" alt="Probability-Probability (pp) Plots." />
<p class="caption">Probability-Probability (pp) Plots.</p>
</div>
<p><strong>KPW</strong> also recommends plotting the difference <span class="math inline">\(D(x) = F_n(x) - F^*(x)\)</span> versus <span class="math inline">\(x\)</span>. Here, <span class="math inline">\(F^*(x)\)</span> is the fitted model distribution function.</p>
</div>
<div id="qq-plot" class="section level4">
<h4><span class="header-section-number">4.2.1.3</span> <em>QQ</em> Plot</h4>
<ul>
<li><p>The horizontal axes gives the empirical quantiles at each observation.</p></li>
<li><p>The right-hand panels they are graphed on a logarithmic basis.</p></li>
<li><p>The vertical axis gives the quantiles from the fitted distributions; Gamma quantiles are in the upper panels, Pareto quantiles are in the lower panels.</p></li>
<li><p>The lower-right hand panel suggests that the Pareto distribution does a good job with large observations but provides a poorer fit for small observations.</p></li>
</ul>
<div class="figure">
<img src="FiguresCh4/QQPlot.jpg" alt="Quantile-Quantile (qq) Plots" />
<p class="caption">Quantile-Quantile (<span class="math inline">\(qq\)</span>) Plots</p>
</div>
</div>
</div>
<div id="statistical-comparisions" class="section level3">
<h3><span class="header-section-number">4.2.2</span> Statistical Comparisions</h3>
<div id="three-goodness-of-fit-statistics" class="section level4">
<h4><span class="header-section-number">4.2.2.1</span> Three Goodness of Fit Statistics</h4>
<p><span class="math display">\[
\begin{matrix}
\begin{array}{ccc}
\text{Statistic} &amp; \text{Definition} &amp; \text{Computational Expression} \\ \hline
Kolmogorov &amp; sup_x |F_n(x) - F(x) |  &amp; max(D^+ - D^-) \\
 -Smirnov &amp;&amp;D^+ = \max_{i=1, \ldots, n} \left(\frac{i}{n} - F_i\right) \\
&amp; &amp;D^- = \max_{i=1, \ldots, n} \left(F_i - \frac{i-1}{n} \right) \\ \hline
Cramer&amp;  n \int (F_n(x) - F(x))^2 dx &amp;
\frac{1}{12n} + \sum_{i=1}^n \left(F_i - (2i-1)/n\right)^2 \\ 
 -von Mises &amp; &amp; \\ \hline
Anderson&amp;  n \int \frac{(F_n(x) - F(x))^2}{F(x)(1-F(x))} dx &amp;\\
  -Darling &amp; &amp;
-\frac{1}{n} \sum_{i=1}^n (2i-1) \log\left(F_i(1-F_{n+1-i})\right)^2 \\ \hline
\end{array}
\end{matrix}
\]</span></p>
</div>
</div>
</div>
<div id="nonparametric-estimation-using-modified-data" class="section level2">
<h2><span class="header-section-number">4.3</span> Nonparametric Estimation using Modified Data</h2>
<div id="grouped-data" class="section level3">
<h3><span class="header-section-number">4.3.1</span> Grouped Data</h3>
<div id="grouped-data-1" class="section level4">
<h4><span class="header-section-number">4.3.1.1</span> Grouped Data</h4>
<ul>
<li><p>Observations may be grouped in the sense that they fall into intervals of the form <span class="math inline">\([c_{j-1}, c_j)\)</span>, for <span class="math inline">\(j=1, \ldots, k\)</span>.</p></li>
<li><p>The constants <span class="math inline">\(\{c_0 &lt; c_1 &lt; \cdots &lt; c_k\}\)</span> form some partition of the domain of F(.).</p></li>
<li><p>Define the empirical distribution function at the boundaries is defined in the usual way: <span class="math display">\[F_n(c_j) = \frac{\text{number of observations } \le c_j}{n}\]</span></p></li>
<li><p>For other values of <span class="math inline">\(x\)</span>, one could use the</p>
<p><strong>Ogive:</strong> connect values of the boundaries with a straight line.</p></li>
<li><p>For another way of smoothing, recall the kernel density estimator of the distribution function <span class="math display">\[\begin{aligned}
 \hat{F}_n(x) = \frac{1}{n} \sum_{i=1}^n K\left(\frac{x-X_i}{b}\right).\end{aligned}\]</span></p></li>
<li><p>For densities, use <span class="math display">\[f_n(x) = \frac{n_j}{n \times (c_j - c_{j-1})}  \ \ \ \ \ \ c_{j-1} \le x &lt; c_j,\]</span></p></li>
</ul>
</div>
</div>
<div id="censored-data" class="section level3">
<h3><span class="header-section-number">4.3.2</span> Censored Data</h3>
<div id="censored-data-1" class="section level4">
<h4><span class="header-section-number">4.3.2.1</span> Censored Data</h4>
<ul>
<li><p>Censoring occurs when we observe only a limited value of an observation.</p></li>
<li><p>Suppose that <span class="math inline">\(X\)</span> represents a loss due to an insured event and that <span class="math inline">\(u\)</span> is a known censoring point.</p></li>
<li><p>If observations are censored from the <strong>right</strong> (or from above), then we observe <span class="math display">\[Y = \min(X,u).\]</span></p>
<ul>
<li>In this case, <span class="math inline">\(u\)</span> may represent the upper limit of coverage for an insurer. The loss exceeds the amount <span class="math inline">\(u\)</span> but the insurer does not have in its records the amount of the actual loss.</li>
</ul></li>
<li><p>If observations are censored from the <strong>left</strong> (or from below), then we observe <span class="math display">\[Y = \max(X,u).\]</span></p>
<ul>
<li>Let <span class="math inline">\(u\)</span> represents the upper limit of coverage but now <span class="math inline">\(Y - u\)</span> represents the amount that a <em>reinsurer</em> is responsible for. If the loss <span class="math inline">\(X &lt; u\)</span>, then <span class="math inline">\(Y=0\)</span>, no loss for the reinsurer. If the loss <span class="math inline">\(X \ge u\)</span>, then <span class="math inline">\(Y= X-u\)</span> represents the reinsurer’s retained claims.</li>
</ul></li>
</ul>
</div>
<div id="kaplan-meier-product-limit-estimator" class="section level4">
<h4><span class="header-section-number">4.3.2.2</span> Kaplan-Meier Product Limit Estimator</h4>
<ul>
<li><p>Let <span class="math inline">\(t_{1} &lt;\cdots&lt; t_{c}\)</span> be distinct points at which an event of interest occurs, or non-censored losses, and let <span class="math inline">\(s_j\)</span> be the number of events at time point <span class="math inline">\(t_{j}\)</span> .</p></li>
<li><p>Further, the corresponding risk set is the number of observations that are active at an instant just prior to <span class="math inline">\(t_{j}\)</span> . Using notation, the risk set is <span class="math inline">\(R_{j}=\sum_{i=1}^{n}I(x_{i}\geq t_{j})\)</span>.</p></li>
<li><p>With this notation, the <strong>product-limit estimator</strong> of the distribution function is <span class="math display">\[\hat{F}(x)=
\left\lbrace
\begin{array}{llll}
0 &amp;
x &lt; t_{1} \\
1-\prod_{j:t_{j} \leq x}\left( 1-\frac{s_j}{R_{j}}\right)  &amp;
x \geq t_{1} .\\
\end{array}
\right .\]</span></p></li>
<li><p>Greenwood (1926) derived the formula for the estimated variance <span class="math display">\[\widehat{Var}(\hat{F}(x)) =
(1-\hat{F}(x))^{2}
\sum _{j:t_{j} \leq x} \dfrac{s_j}{R_{j}(R_{j}-s_j)}.\]</span></p></li>
</ul>
</div>
</div>
<div id="truncated-data" class="section level3">
<h3><span class="header-section-number">4.3.3</span> Truncated Data</h3>
<div id="truncated-data-1" class="section level4">
<h4><span class="header-section-number">4.3.3.1</span> Truncated Data</h4>
<ul>
<li><p>An outcome is potentially <strong>truncated</strong> when the availability of an observation depends on the outcome.</p></li>
<li><p>In insurance, it is common for observations to be truncated from the <strong>left</strong> (or below) at <span class="math inline">\(d\)</span> when the amount observed is <span class="math display">\[Y = \begin{cases}
\text{we do not observe X}  &amp;  X &lt; d\\
X-d                         &amp;   X \ge d.
\end{cases}\]</span></p>
<ul>
<li>In this case, <span class="math inline">\(d\)</span> may represent the deductible associated with an insurance coverage. If the insured loss is less than the deductible, then the insurer does not observe the loss. If the loss exceeds the deductible, then the excess <span class="math inline">\(X-d\)</span> is the claim that the insurer covers.</li>
</ul></li>
<li><p>Observations may also truncated from the <strong>right</strong> (or above) at <span class="math inline">\(d\)</span> when the amount observed is <span class="math display">\[Y = \begin{cases}
X   &amp;   X &lt; d  \\
\text{we do not observe X}  &amp;  X \ge d\\
\end{cases}\]</span></p>
<ul>
<li>Classic examples of truncation from the right include <span class="math inline">\(X\)</span> as a measure of distance of a star. When the distance exceeds a certain level <span class="math inline">\(d\)</span>, the star is no longer observable.</li>
</ul></li>
</ul>
</div>
<div id="right-censored-left-truncated-empirical-distribution-functions" class="section level4">
<h4><span class="header-section-number">4.3.3.2</span> Right-Censored, Left-Truncated Empirical Distribution Functions</h4>
<ul>
<li><p>Procedure from <strong>KPW</strong>. Notation:</p>
<ul>
<li><p>For each observation <span class="math inline">\(i\)</span>, let <span class="math inline">\(d_i\)</span> be the lower truncation limit (0 if no truncation)</p></li>
<li><p>Let <span class="math inline">\(u_i\)</span> be the upper censoring limit (=<span class="math inline">\(\infty\)</span> if no censoring)</p></li>
<li><p>The recorded value is <span class="math inline">\(x_i\)</span> in the case of no censoring, <span class="math inline">\(u_i\)</span> if there is censoring.</p></li>
<li><p>For notation, let <span class="math inline">\(t_1 &lt; \cdots &lt; t_k\)</span> be <span class="math inline">\(k\)</span> unique observations of <span class="math inline">\(x_i\)</span> that are uncensored.</p></li>
<li><p>Define <span class="math inline">\(s_j\)</span> to be the number of <span class="math inline">\(x_i\)</span>’s at <span class="math inline">\(t_j\)</span>.</p></li>
<li><p>Define the risk set <span class="math display">\[R_j = \sum_{i=1}^n I(x_i \geq t_{j}) + \sum_{i=1}^n I(u_i \geq t_{j}) - \sum_{i=1}^n I(d_i \geq t_{j})\]</span></p></li>
</ul></li>
<li><p>The product-limit estimator of the distribution function is <span class="math display">\[\hat{F}(x)=
\left\lbrace
\begin{array}{llll}
0 &amp;
x &lt; t_{1} \\
1- \prod_{j:t_{j} \leq x}\left( 1-\frac{s_j}{R_{j}}\right)  &amp;
x \geq t_{1}\\
\end{array}
\right .\]</span></p></li>
<li><p>The Nelson-Aalen estimator of the distribution function is <span class="math display">\[\hat{F}(x)=
\left\lbrace
\begin{array}{llll}
0 &amp;
x &lt; t_{1} \\
1- \exp \left(-\sum_{j:t_{j} \leq x}\frac{s_j}{R_j} \right) &amp;
x \geq t_{1}\\
\end{array}
\right.\]</span></p></li>
</ul>
</div>
</div>
</div>
<div id="topics-in-parametric-estimation" class="section level2">
<h2><span class="header-section-number">4.4</span> Topics in Parametric Estimation</h2>
<div id="starting-values" class="section level3">
<h3><span class="header-section-number">4.4.1</span> Starting Values</h3>
<ul>
<li><p>Maximum likelihood is a desirable estimation technique because</p>
<ul>
<li><p>It employs data efficiently (enjoys certain optimality properties)</p></li>
<li><p>It can be used in a variety of data sampling schemes (e.g.,<em>iid</em>, grouped, censored, regression, and so forth)</p></li>
</ul></li>
<li><p>However, maximum likelihood is a recursive estimation procedure that requires starting values to begin the recursion</p></li>
<li><p>Two alternative estimation techniques are:</p>
<ul>
<li><p>Method of moments</p></li>
<li><p>Percentile matching</p></li>
</ul></li>
<li><p>These are non-recursive techniques. Easy to implement and explain. Although less efficient than maximum likelihood, they can be employed to provide starting values for maximum likelihood.</p></li>
</ul>
<div id="method-of-moments" class="section level4">
<h4><span class="header-section-number">4.4.1.1</span> Method of Moments</h4>
<ul>
<li><p>Idea: Approximate the moments using a parametric distribution to the empirical (nonparametric) moments</p></li>
<li><p><strong>Example - Property Fund.</strong> For the 2010 property fund, there are <span class="math inline">\(n=1,377\)</span> individual claims (in thousands of dollars) with <span class="math display">\[\begin{aligned}
m_1 = \frac{1}{n} \sum_{i=1}^n X_i = 26.62259 \ \ \ \
\text{and} \ \ \ \
 m_2 = \frac{1}{n} \sum_{i=1}^n X_i^2 = 136154.6 .\end{aligned}\]</span></p></li>
<li><p>Gamma Distribution</p>
<ul>
<li><p>From theory, <span class="math inline">\(\mu_1 = \alpha \theta\)</span> and <span class="math inline">\(\mu_2^{\prime} = \alpha(\alpha+1) \theta^2\)</span>.</p></li>
<li><p>Equating the two yields the method of moments estimators, easy algebra shows that <span class="math display">\[\begin{aligned}
\alpha = \frac{\mu_1^2}{\mu_2^{\prime}-2\mu_1^2}  \ \ \ \text{and} \ \ \  \theta = \frac{\mu_2^{\prime}-\mu_1^2}{\mu_1}.\end{aligned}\]</span></p></li>
<li><p>The method of moment estimators are <span class="math display">\[\begin{aligned}
\hat{\alpha} &amp;= \frac{26.62259^2}{136154.6-26.62259^2} = 0.005232809\\
%\text{and} \\
\hat{\theta} &amp;= \frac{136154.6-26.62259^2}{26.62259} = 5,087.629.\end{aligned}\]</span></p></li>
<li><p>In contrast, the maximum likelihood values turn out to be <span class="math inline">\(\hat{\alpha}_{MLE} = 0.2905959\)</span> and <span class="math inline">\(\hat{\theta}_{MLE} = 91.61378\)</span></p></li>
<li><p>Big discrepancies between the two estimation procedures, suggesting that the gamma model fits poorly.</p></li>
</ul></li>
<li><p><strong>Example - Property Fund.</strong> Recall the nonparametric estimates <span class="math display">\[\begin{aligned}
m_1 = \frac{1}{n} \sum_{i=1}^n X_i = 26.62259 \ \ \ \
\text{and} \ \ \ \
 m_2 = \frac{1}{n} \sum_{i=1}^n X_i^2 = 136154.6 .\end{aligned}\]</span></p></li>
<li><p>Pareto Distribution</p>
<ul>
<li><p>From theory, <span class="math inline">\(\mu_1 = \theta/(\alpha -1)\)</span> and <span class="math inline">\(\mu_2^{\prime} = 2\theta^2/((\alpha-1)(\alpha-2) )\)</span>.</p></li>
<li><p>Easy algebra shows <span class="math display">\[\begin{aligned}
\alpha = 1+ \frac{\mu_2^{\prime}}{\mu_2^{\prime}-2\mu_1^2} \ \ \ \
\text{and} \ \ \ \ \
 \theta = (\alpha-1)\mu_1.\end{aligned}\]</span></p></li>
<li><p>The method of moment estimators are <span class="math display">\[\begin{aligned}
\hat{\alpha} &amp;= 1+ \frac{136154.6}{136154.6-2*26,62259^2} = 2.01052\\
%\text{and} \\
\hat{\theta} &amp;= (2.01052-1) \cdot 26.62259 = 26.9027\end{aligned}\]</span></p></li>
<li><p>The maximum likelihood values turn out to be <span class="math inline">\(\hat{\alpha}_{MLE} = 0.9990936\)</span> and <span class="math inline">\(\hat{\theta}_{MLE} = 2.2821147\)</span>.</p></li>
<li><p>Interesting that <span class="math inline">\(\hat{\alpha}_{MLE}&lt;1\)</span>; for the Pareto distribution; recall that <span class="math inline">\(\alpha &lt;1\)</span> means that the mean is infinite.</p></li>
<li><p>Indicates that the property claims data set is a long tail distribution.</p></li>
</ul></li>
</ul>
</div>
<div id="percentile-matching" class="section level4">
<h4><span class="header-section-number">4.4.1.2</span> Percentile Matching</h4>
<ul>
<li><p>Under percentile matching, one approximates the parametric distribution using the empirical (nonparametric) quantiles, or percentiles.</p></li>
<li><p><strong>Example - Property Fund.</strong> The 25th percentile (the first quartile) turns out to be 0.78853 and the 95th percentile is 50.98293 (both in thousands of dollars).</p></li>
<li><p>Pareto Distribution</p>
<ul>
<li><p>The Pareto distribution is particularly intuitively pleasing because of the closed-form solution for the quantiles.</p></li>
<li><p>The distribution function is <span class="math inline">\(F(x) = 1 - \left(\theta/(x+\theta )\right)^{\alpha}\)</span>.</p></li>
<li><p>Easy algebra shows that we can express the quantile as <span class="math display">\[F^{-1}(q) = \theta \left( (1-q)^{-1/\alpha} -1 \right)\]</span> for a fraction <span class="math inline">\(q\)</span>, <span class="math inline">\(0&lt;q&lt;1\)</span>.</p></li>
<li><p>With two equations <span class="math display">\[0.78853 = \theta \left( (1-.25)^{-1/\alpha} -1 \right) \ \ \ \ \text{and} \ \ \ \ 50.98293 = \theta \left( (1-.95)^{-1/\alpha} -1\right)\]</span> and two unknowns, the solution is <span class="math display">\[
\hat{\alpha} = 0.9412076 \ \ \ \ \ \text{and} \ \ \ \
\hat{\theta} = 2.205617 .\]</span></p></li>
<li><p>A numerical routine was required for these solutions - no analytic solution available.</p></li>
<li><p>Recall that the maximum likelihood values are <span class="math inline">\(\hat{\alpha}_{MLE} = 0.9990936\)</span> and <span class="math inline">\(\hat{\theta}_{MLE} = 2.2821147\)</span>.</p></li>
<li><p>The percentile matching provides a better approximation for the Pareto distribution than did the method of moments.</p></li>
</ul></li>
</ul>
</div>
</div>
<div id="grouped-data-2" class="section level3">
<h3><span class="header-section-number">4.4.2</span> Grouped Data</h3>
<div id="parametric-estimation-using-grouped-data" class="section level4">
<h4><span class="header-section-number">4.4.2.1</span> Parametric Estimation Using Grouped Data</h4>
<ul>
<li><p>Observations may be grouped in the sense that they fall into intervals of the form <span class="math inline">\((c_{j-1}, c_j]\)</span>, for <span class="math inline">\(j=1, \ldots, k\)</span>.</p></li>
<li><p>The constants <span class="math inline">\(\{c_0 &lt; c_1 &lt; \cdots &lt; c_k\}\)</span> form some partition of the domain of F(.).</p></li>
<li><p>Define <span class="math inline">\(n_j\)</span> to be the number of observations that fall in the <span class="math inline">\(j\)</span>th interval, <span class="math inline">\((c_{j-1}, c_j]\)</span>.</p></li>
<li><p>The probability of an observation <span class="math inline">\(X\)</span> falling in the <span class="math inline">\(j\)</span>th interval is <span class="math display">\[\Pr\left(X \in c_{j-1}, c_j]\right) = F(c_j) - F(c_{j-1}).\]</span></p></li>
<li><p>The probability of an observation <span class="math inline">\(X\)</span> falling in the <span class="math inline">\(j\)</span>th interval is <span class="math display">\[\Pr\left(X \in c_{j-1}, c_j]\right) = F(c_j) - F(c_{j-1}).\]</span></p></li>
<li><p>The corresponding mass function is <span class="math display">\[\begin{aligned}
f(x) &amp;=
\begin{cases}
F(c_1) - F(c_{0}) &amp;   \textrm{if~} x \in (c_{0}, c_1]\\
\vdots &amp; \vdots \\
F(c_k) - F(c_{k-1}) &amp;   \textrm{if~} x \in (c_{k-1}, c_k]\\
\end{cases} \\
&amp;= \prod_{j=1}^k \left\{F(c_j) - F(c_{j-1})\right\}^{I(x \in (c_{j-1}, c_j])}\end{aligned}\]</span></p></li>
<li><p>The likelihood is <span class="math display">\[\begin{aligned}
\prod_{j=1}^n f(x_i) = \prod_{j=1}^k \left\{F(c_j) - F(c_{j-1})\right\}^{n_j}\end{aligned}\]</span></p></li>
<li><p>The log-likelihood is <span class="math display">\[\begin{aligned}
L(\theta) = \ln \prod_{j=1}^n f(x_i) = \sum_{j=1}^k n_j \ln \left\{F(c_j) - F(c_{j-1})\right\}\end{aligned}\]</span></p></li>
</ul>
</div>
</div>
<div id="parametric-estimation-using-censored-data" class="section level3">
<h3><span class="header-section-number">4.4.3</span> Parametric Estimation Using Censored Data</h3>
<div id="censored-data-likelihood" class="section level4">
<h4><span class="header-section-number">4.4.3.1</span> Censored Data Likelihood</h4>
<ul>
<li><p>Censoring occurs when we observe only a limited value of an observation.</p></li>
<li><p>Suppose that <span class="math inline">\(X\)</span> represents a loss due to an insured event and that <span class="math inline">\(u\)</span> is a known censoring point.</p></li>
<li><p>If observations are censored from the <strong>right</strong> (or from above), then we observe we observe <span class="math inline">\(Y= \min(X, u)\)</span> and <span class="math inline">\(\delta_u= \mathrm{I}(X \geq u)\)</span>.</p></li>
<li><p>If censoring occurs so that <span class="math inline">\(\delta_u=1\)</span>, then <span class="math inline">\(X \geq u\)</span> and the likelihood is <span class="math inline">\(\Pr(X \ge u) = 1-\mathrm{F}(u)\)</span>.</p></li>
<li><p>If censoring does not occur so that <span class="math inline">\(\delta_u=0\)</span>, then <span class="math inline">\(X &lt; C_U\)</span> and the likelihood is <span class="math inline">\(\mathrm{f}(y)\)</span>.</p></li>
<li><p>Summarizing, we have <span class="math display">\[\begin{aligned}
Likelihood  &amp;= \left\{
\begin{array}{cl}
\mathrm{f}(y) &amp; \textrm{if~}\delta=0 \\
1-\mathrm{F}(u)  &amp;  \textrm{if~}\delta=1
\end{array}\right. \\
&amp;= \left( \mathrm{f}(y)\right)^{1-\delta} \left(1-\mathrm{F}(u)\right)^{\delta} .\end{aligned}\]</span> The second right-hand expression allows us to present the likelihood more compactly.</p></li>
<li><p>For a single observation, we have <span class="math display">\[\begin{aligned}
Likelihood  &amp;= \left\{
\begin{array}{cl}
\mathrm{f}(y) &amp; \textrm{if~}\delta=0 \\
1-\mathrm{F}(u)  &amp;  \textrm{if~}\delta=1
\end{array}\right. \\
&amp;= \left( \mathrm{f}(y)\right)^{1-\delta} \left(1-\mathrm{F}(u)\right)^{\delta} .\end{aligned}\]</span></p></li>
<li><p>Consider a random sample of size <span class="math inline">\(n\)</span>, <span class="math display">\[\{ (y_1,\delta_1), \ldots,(y_n, \delta_n) \} \]</span> with potential censoring times ${ u_1, , u_n } $.</p></li>
<li><p>The likelihood is <span class="math display">\[\prod_{i=1}^n \left( \mathrm{f}(y_i)\right)^{1-\delta_i} \left(1-\mathrm{F}(u_i)\right)^{\delta_i}
= \prod_{\delta_i=0}\mathrm{f}(y_i) \prod_{\delta_i=1} \{1-\mathrm{F}(u_i)\},\]</span></p></li>
<li><p>Here, the notation <span class="math inline">\(\prod_{\delta_i=0}\)</span> means take the product over uncensored observations, and similarly for <span class="math inline">\(\prod_{\delta_i=1}\)</span>.</p></li>
<li><p>The log-likelihood is <span class="math display">\[L(\theta) = \sum_{i=1}^n \left\{(1-\delta_i) \ln  \mathrm{f}(y_i) +  \delta_i \ln \left(1-\mathrm{F}(u_i)\right) \right\}\]</span></p></li>
</ul>
</div>
</div>
<div id="censored-and-truncated-data" class="section level3">
<h3><span class="header-section-number">4.4.4</span> Censored and Truncated Data</h3>
<ul>
<li>Let <span class="math inline">\(X\)</span> denote the outcome and let <span class="math inline">\(C_L\)</span> and <span class="math inline">\(C_U\)</span> be two constants.</li>
</ul>
<table>
<thead>
<tr class="header">
<th align="left">Type</th>
<th align="center">Limited Variable</th>
<th align="center">Censoring Information</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">right censoring</td>
<td align="center"><span class="math inline">\(X_U^{\ast}= \min(X, C_U)\)</span></td>
<td align="center"><span class="math inline">\(\delta_U= \mathrm{I}(X \geq C_U)\)</span></td>
</tr>
<tr class="even">
<td align="left">left censoring</td>
<td align="center"><span class="math inline">\(X_L^{\ast}= \max(y, C_L)\)</span></td>
<td align="center"><span class="math inline">\(\delta_L= \mathrm{I}(X \leq C_L)\)</span></td>
</tr>
<tr class="odd">
<td align="left">interval censoring</td>
<td align="center"></td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="left">right truncation</td>
<td align="center"><span class="math inline">\(X\)</span></td>
<td align="center">observe <span class="math inline">\(X\)</span> if <span class="math inline">\(X &lt; C_U\)</span></td>
</tr>
<tr class="odd">
<td align="left">left truncation</td>
<td align="center"><span class="math inline">\(X\)</span></td>
<td align="center">observe <span class="math inline">\(X\)</span> if <span class="math inline">\(X &lt; C_L\)</span></td>
</tr>
</tbody>
</table>
<div class="figure" style="text-align: center"><span id="fig:CensoringTruncation"></span>
<img src="LossDataAnalytics_files/figure-html/CensoringTruncation-1.png" alt="Censoring and Truncation" width="80%" />
<p class="caption">
Figure 4.2: Censoring and Truncation
</p>
</div>
<div id="example-mortality-study" class="section level4">
<h4><span class="header-section-number">4.4.4.1</span> Example: Mortality Study</h4>
<ul>
<li><p>Suppose that you are conducting a two-year study of mortality of high-risk subjects, beginning January 1, 2010 and finishing January 1, 2012.</p></li>
<li><p>For each subject, the beginning of the arrow represents that the the subject was recruited and the arrow end represents the event time. Thus, the arrow represents exposure time.</p></li>
</ul>
<div class="figure" style="text-align: center"><span id="fig:MortalityTimeLine"></span>
<img src="LossDataAnalytics_files/figure-html/MortalityTimeLine-1.png" alt="Subjects on Test in a Mortality Study" width="80%" />
<p class="caption">
Figure 4.3: Subjects on Test in a Mortality Study
</p>
</div>
<ul>
<li><p>Type A - <strong>right-censored</strong>. This subject is alive at the beginning and the end of the study. Because the time of death is not known by the end of the study, it is right-censored. Most subjects are Type A.</p></li>
<li><p>Type B. <strong>Complete information</strong> is available for a type B subject. The subject is alive at the beginning of the study and the death occurs within the observation period.</p></li>
<li><p>Type C - <strong>right-censored</strong> and <strong>left-truncated</strong>. A type C subject is right-censored, in that death occurs after the observation period. However, the subject entered after the start of the study and is said to have a <em>delayed entry time</em>. Because the subject would not have been observed had death occurred before entry, it is left-truncated.</p></li>
<li><p>Type D - <strong>left-truncated</strong>. A type D subject also has delayed entry. Because death occurs within the observation period, this subject is not right censored.</p></li>
<li><p>Type E - <strong>left-truncated</strong>. A type E subject is not included in the study because death occurs prior to the observation period.</p></li>
<li><p>Type F - <strong>right-truncated</strong>. Similarly, a type F subject is not included because the entry time occurs after the observation period.</p></li>
</ul>
</div>
</div>
<div id="parametric-estimation-using-censored-and-truncated-data" class="section level3">
<h3><span class="header-section-number">4.4.5</span> Parametric Estimation Using Censored and Truncated Data</h3>
<ul>
<li><p>Truncated data are handled in likelihood inference via conditional probabilities.</p></li>
<li><p>Adjust the likelihood contribution by dividing by the probability that the variable was observed.</p></li>
<li><p>Summarizing, we have the following contributions to the likelihood for six types of outcomes.</p></li>
</ul>
<p><span class="math display">\[\begin{array}{lc}
\hline Outcome            &amp; Likelihood~Contribution \\\hline
\text{exact value      }  &amp; f(x) \\
\text{right-censoring  }  &amp; 1-F(C_U) \\
\text{left-censoring   }  &amp; F(C_L) \\
\text{right-truncation }  &amp; f(x)/F(C_U) \\
\text{left-truncation    }&amp; f(x)/(1-F(C_L)) \\
\text{interval-censoring} &amp; F(C_U)-F(C_L) \\
\hline
\end{array}\]</span></p>
<ul>
<li><p>For known outcomes and censored data, the likelihood is <span class="math display">\[\prod_{E} \mathrm{f}(x_i) \prod_{R} \{1-\mathrm{F}(C_{Ui})\} \prod_{L}
\mathrm{F}(C_{Li}) \prod_{I} (\mathrm{F}(C_{Ui})-\mathrm{F}(C_{Li})),\]</span> where <span class="math inline">\(\prod_{E}\)</span> is the product over observations with <em>E</em>xact values, and similarly for <em>R</em>ight-, <em>L</em>eft- and <em>I</em>nterval-censoring.</p></li>
<li><p>For right-censored and left-truncated data, the likelihood is <span class="math display">\[\prod_{E} \frac{\mathrm{f}(x_i)}{1-\mathrm{F}(C_{Li})} \prod_{R} \frac{1-\mathrm{F}(C_{Ui})}{1-\mathrm{F}(C_{Li})} ,\]</span></p></li>
<li><p>Similarly for other combinations.</p></li>
</ul>
<div id="special-case-exponential-distribution" class="section level4">
<h4><span class="header-section-number">4.4.5.1</span> Special Case: Exponential Distribution</h4>
<ul>
<li><p>Consider data that are right-censored and left-truncated, with random variables <span class="math inline">\(X_i\)</span> that are exponentially distributed with mean <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>With these specifications, recall that <span class="math inline">\(\mathrm{f}(x) = \theta^{-1} \exp(-x/\theta)\)</span> and <span class="math inline">\(\mathrm{F}(x) = 1-\exp(-x/\theta)\)</span>.</p></li>
<li><p>For this special case, the logarithmic likelihood is <span class="math display">\[\begin{aligned}
 \ln Likelihood  &amp;= \sum_{E} \left( \ln \mathrm{f}(x_i) - \ln (1-\mathrm{F}(C_{Li})) \right) -\sum_{R}\left( \ln (1-\mathrm{F}(C_{Ui}))- \ln (1-\mathrm{F}(C_{Li}))
 \right) \\
 &amp;=  \sum_{E} (-\ln \theta -(x_i-C_{Li})/\theta ) -\sum_{R} (C_{Ui}-C_{Li})/\theta .\end{aligned}\]</span></p></li>
<li><p>To simplify the notation, define <span class="math inline">\(\delta_i = \mathrm{I}(X_i \geq C_{Ui})\)</span> to be a binary variable that indicates right-censoring.</p></li>
<li><p>Let <span class="math inline">\(X_i^{\ast \ast} = \min(X_i, C_{Ui}) - C_{Li}\)</span> be the amount that the observed variable exceeds the lower truncation limit.</p></li>
<li><p>With this, the logarithmic likelihood is <span class="math display">\[\ln Likelihood =  - \sum_{i=1}^n \left((1-\delta_i) \ln \theta + \frac{x_i^{\ast \ast}}{\theta} \right).\]</span></p></li>
<li><p>Taking derivatives with respect to the parameter <span class="math inline">\(\theta\)</span> and setting it equal to zero yields the maximum likelihood estimator <span class="math display">\[\begin{aligned}
\widehat{\theta}  &amp;= \frac{1}{n_u} \sum_{i=1}^n  x_i^{\ast \ast},\end{aligned}\]</span> where <span class="math inline">\(n_u = \sum_i (1-\delta_i)\)</span> is the number of uncensored observations.</p></li>
</ul>
</div>
</div>
</div>
<div id="bayesian-inference" class="section level2">
<h2><span class="header-section-number">4.5</span> Bayesian Inference</h2>
<div id="bayesian-model" class="section level3">
<h3><span class="header-section-number">4.5.1</span> Bayesian Model</h3>
<ul>
<li><p>In the <strong>frequentist interpretation</strong>, one treats the vector of parameters <span class="math inline">\(\boldsymbol \theta\)</span> as fixed yet unknown, whereas the outcomes <span class="math inline">\(X\)</span> are realizations of random variables.</p></li>
<li><p>With Bayesian statistical models, one views both the model parameters and the data as random variables.</p>
<ul>
<li>Use probability tools to reflect this uncertainty about the parameters <span class="math inline">\(\boldsymbol \theta\)</span>.</li>
</ul></li>
<li><p>For notation, we will think about <span class="math inline">\(\boldsymbol \theta\)</span> as a random vector and let <span class="math inline">\(\pi(\boldsymbol \theta)\)</span> denote the distribution of possible outcomes.</p></li>
</ul>
<div id="bayesian-inference-strengths" class="section level4">
<h4><span class="header-section-number">4.5.1.1</span> Bayesian Inference Strengths</h4>
<p>There are several advantages of the Bayesian approach.</p>
<ol style="list-style-type: decimal">
<li><p>One can describe the entire distribution of parameters conditional on the data. This allows one, for example, to provide probability statements regarding the likelihood of parameters.</p></li>
<li><p>This approach allows analysts to blend information known from other sources with the data in a coherent manner. This topic is developed in detail in the credibility chapter.</p></li>
<li><p>The Bayesian approach provides for a unified approach for estimating parameters. Some non-Bayesian methods, such as least squares, required a approach to estimating variance components. In contrast, in Bayesian methods, all parameters can be treated in a similar fashion. Convenient for explaining results to consumers of the data analysis.</p></li>
<li><p>Bayesian analysis is particularly useful for forecasting future responses.</p></li>
</ol>
</div>
<div id="bayesian-model-1" class="section level4">
<h4><span class="header-section-number">4.5.1.2</span> Bayesian Model</h4>
<ul>
<li><p><strong>Prior Distribution.</strong> <span class="math inline">\(\pi(\boldsymbol \theta)\)</span> is called the <em>prior distribution</em>.</p>
<ul>
<li><p>Typically, it is a regular distribution and so integrates to one.</p></li>
<li><p>We may be very uncertain (or have no clue) about the distribution of <span class="math inline">\(\boldsymbol \theta\)</span>; the Bayesian machinery allows this situation <span class="math display">\[\int \pi(\theta) d\theta = \infty\]</span> in which case <span class="math inline">\(\pi(\cdot)\)</span> is called an <strong>improper prior</strong>.</p></li>
</ul></li>
<li><p><strong>Model Distribution.</strong> The distribution of outcomes given an assumed value of <span class="math inline">\(\boldsymbol \theta\)</span> is known as the <em>model distribution</em> and denoted as <span class="math inline">\(f(x | \boldsymbol \theta) = f_{X|\boldsymbol \theta} (x|\boldsymbol \theta )\)</span>. This is the (usual frequentist) mass or density function.</p></li>
<li><p><strong>Joint Distribution.</strong> The distribution of outcomes and model parameters is, not surprisingly, known as the <em>joint distribution</em> and denoted as <span class="math inline">\(f(x , \boldsymbol \theta) = f(x|\boldsymbol \theta )\pi(\boldsymbol \theta)\)</span>.</p></li>
<li><p><strong>Marginal Outcome Distribution.</strong> The distribution of outcomes can be expressed as <span class="math display">\[f(x) =\int f(x | \boldsymbol \theta)\pi(\boldsymbol \theta) d\boldsymbol \theta.\]</span> This is analogous to a frequentist mixture distribution.</p></li>
<li><p><strong>Posterior Distribution of Parameters.</strong> After outcomes have been observed (hence the terminology posterior), one can use Bayes theorem to write the distribution as <span class="math display">\[\pi(\boldsymbol \theta | x) =\frac{f(x , \boldsymbol \theta)}{f(x)} =\frac{f(x|\boldsymbol \theta )\pi(\boldsymbol \theta)}{f(x)}\]</span> The idea is to update your knowledge of the distribution of <span class="math inline">\(\boldsymbol \theta\)</span> (<span class="math inline">\(\pi(\boldsymbol \theta)\)</span>) with the data <span class="math inline">\(x\)</span>.</p>
<ul>
<li><p>We can summarize the distribution using a confidence interval type statement.</p></li>
<li><p><strong>Definition</strong>. <span class="math inline">\([a,b]\)</span> is said to be a <span class="math inline">\(100(1-\alpha)\%\)</span> <strong>credibility interval</strong> for <span class="math inline">\(\boldsymbol \theta\)</span> if <span class="math display">\[\Pr (a \le \theta \le b | \mathbf{x}) \ge 1- \alpha.\]</span></p></li>
</ul></li>
</ul>
</div>
<div id="two-examples" class="section level4">
<h4><span class="header-section-number">4.5.1.3</span> Two Examples</h4>
<p><strong>Exam C Question 157.</strong> You are given:<br />
(i) In a portfolio of risks, each policyholder can have at most one claim per year.<br />
(ii) The probability of a claim for a policyholder during a year is <span class="math inline">\(q\)</span>.<br />
(iii) The prior density is <span class="math display">\[\pi(q) = q^3/0.07, \ \ \ 0.6 &lt; q &lt; 0.8\]</span> A randomly selected policyholder has one claim in Year 1 and zero claims in Year 2.<br />
For this policyholder, calculate the posterior probability that <span class="math inline">\(0.7 &lt; q &lt; 0.8\)</span>.</p>
<p><strong>Exam C Question 43.</strong> You are given:<br />
(i) The prior distribution of the parameter <span class="math inline">\(\Theta\)</span> has probability density function: <span class="math display">\[\pi(\theta) = 1/\theta^2, \ \ \ \ 1 &lt; \theta &lt; \infty\]</span> (ii) Given <span class="math inline">\(\Theta = \theta\)</span>, claim sizes follow a Pareto distribution with parameters <span class="math inline">\(\alpha=2\)</span> and <span class="math inline">\(\theta\)</span>.<br />
A claim of 3 is observed.<br />
Calculate the posterior probability that <span class="math inline">\(\Theta\)</span> exceeds 2.</p>
</div>
</div>
<div id="bayesian-inference---decision-analysis" class="section level3">
<h3><span class="header-section-number">4.5.2</span> Bayesian Inference - Decision Analysis</h3>
<ul>
<li><p>In classical decision analysis, the loss function <span class="math inline">\(l(\hat{\theta}, \theta)\)</span> determines the penalty paid for using the estimate <span class="math inline">\(\hat{\theta}\)</span> instead of the true <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>The <strong>Bayes estimate</strong> is that value that minimizes the expected loss <span class="math inline">\(\mathrm{E~}l(\hat{\theta}, \theta)\)</span>.</p></li>
<li><p>Some important special cases include: <span class="math display">\[\begin{array}{ccc}  \hline
\text{ Loss function } l(\hat{\theta}, \theta) &amp; \text{Descriptor} &amp; \text{Bayes Estimate}\\ \hline
(\hat{\theta}- \theta)^2 &amp; \text{squared error loss} &amp; \mathrm{E}(\theta|X)  \\
|\hat{\theta}- \theta| &amp; \text{absolute deviation loss} &amp; median of \pi(\theta|x)\\
I(\hat{\theta} =\theta) &amp; \text{zero-one loss (for discrete probabilities)}&amp; mode of \pi(\theta|x) \\  \hline
\end{array}\]</span></p></li>
<li><p>For new data <span class="math inline">\(y\)</span>, the predictive distribution is <span class="math display">\[f(y|x) = \int f(y|\theta) \pi(\theta|x) d\theta .\]</span></p></li>
<li><p>With this, the Bayesian prediction of <span class="math inline">\(y\)</span> is <span class="math display">\[\begin{aligned}
\mathrm{E}(y|x) &amp;= \int y f(y|x) dy = \int y \left(\int f(y|\theta) \pi(\theta|x) d\theta \right) dy \\
&amp;= \int  \mathrm{E}(y|\theta) \pi(\theta|x) d\theta .\end{aligned}\]</span></p></li>
</ul>
<div id="posterior-distribution" class="section level4">
<h4><span class="header-section-number">4.5.2.1</span> Posterior Distribution</h4>
<p>How to calculate the posterior distribution?</p>
<ul>
<li><p><strong>By hand</strong> - can do this in special cases</p></li>
<li><p><strong>Simulation</strong> - uses modern computational techniques. <strong>KPW</strong> (Section 12.4.4) mentions Markov Chain Monte Carlo (MCMC) simulation</p></li>
<li><p><strong>Normal Approximation</strong>. Theorem 12.39 of <strong>KPW</strong> provides a justification</p></li>
<li><p><strong>Conjugate distributions</strong>. Classical approach. Although this approach is available only for a limited number of distributions, it has the appeal that it provides closed-form expressions for the distributions, allowing for easy interpretations of results. We focus on this approach.</p></li>
</ul>
<p>To relate the prior and posterior distributions of the parameters, we have <span class="math display">\[\begin{aligned}
\pi(\boldsymbol \theta | x)&amp;=\frac{f(x|\boldsymbol \theta )\pi(\boldsymbol \theta)}{f(x)} \\
&amp; \propto  f(x|\boldsymbol \theta ) \pi(\boldsymbol \theta) \\
\text{Posterior} &amp;\text{is proportional to} \text{likelihood} \times \text{prior}
\end{aligned}\]</span></p>
<p>For <strong>conjugate distributions</strong>, the posterior and the prior come from the same family of distributions.</p>
</div>
<div id="special-case-poisson-gamma-conjugate-family" class="section level4">
<h4><span class="header-section-number">4.5.2.2</span> Special Case: Poisson Gamma Conjugate Family</h4>
<ul>
<li><p>Assume a Poisson(<span class="math inline">\(\lambda\)</span>) model distribution so that <span class="math display">\[f(\mathbf{x} | \lambda) = \prod_{i=1}^n \frac{\lambda^{x_i} e^{-\lambda}}{x_i!}\]</span></p></li>
<li><p>Assume <span class="math inline">\(\lambda\)</span> follows a gamma(<span class="math inline">\(\alpha, \theta\)</span>) prior distribution so that <span class="math display">\[\pi(\lambda) = \frac{\left(\lambda/\theta\right)^{\alpha} \exp(-\lambda/\theta)}{\lambda \Gamma(\alpha)}.\]</span></p></li>
<li><p>The posterior distribution is proportional to <span class="math display">\[\begin{aligned}
\pi(\lambda | \mathbf{x}) &amp; \propto f(\mathbf{x}|\theta ) \pi(\lambda) \\
&amp;= C \lambda^{\sum_i x_i + \alpha -1} \exp(-\lambda (n+1/\theta))\end{aligned}\]</span> where <span class="math inline">\(C\)</span> is a constant.</p></li>
<li><p>We recognize this to be a gamma distribution with new parameters <span class="math inline">\(\alpha_{new} = \sum_i x_i + \alpha\)</span> and <span class="math inline">\(\theta_{new} = 1/(n + 1/\theta)\)</span>.</p></li>
</ul>
</div>
</div>
</div>
<div id="exercises-2" class="section level2">
<h2><span class="header-section-number">4.6</span> Exercises</h2>
<p>Here are a set of exercises that guide the viewer through some of the theoretical foundations of <strong>Loss Data Analytics</strong>. Each tutorial is based on one or more questions from the professional actuarial examinations – typically the Society of Actuaries Exam C.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knitr::<span class="kw">include_url</span>(<span class="st">&quot;http://www.ssc.wisc.edu/~jfrees/loss-data-analytics/loss-data-analytics-model-selection/&quot;</span>,<span class="dt">height =</span> <span class="st">&quot;600px&quot;</span>)</code></pre></div>
<iframe src="http://www.ssc.wisc.edu/~jfrees/loss-data-analytics/loss-data-analytics-model-selection/" width="672" height="600px">
</iframe>

</div>
</div>
<div id="disqus_thread"></div>
<script>

/**
*  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
*  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
/*
var disqus_config = function () {
this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
*/
(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = '//lossdataanalytics.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
            </section>

          </div>
        </div>
      </div>
<a href="modeling-loss-severity.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="simulation.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/Chapters/ModelSelection28Jan2017.Rmd",
"text": "Edit"
},
"download": ["LossDataAnalytics.pdf", "LossDataAnalytics.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
